apiVersion: apps/v1
kind: Deployment
metadata:
  name: llamacpp
  namespace: llama-app
  labels:
    app: llamacpp
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1
  selector:
    matchLabels:
      app: llamacpp
  template:
    metadata:
      labels:
        app: llamacpp
    spec:
      # Speed up pod scheduling
      tolerations:
      - key: "node.kubernetes.io/not-ready"
        operator: "Exists"
        effect: "NoExecute"
        tolerationSeconds: 30
      - key: "node.kubernetes.io/unreachable"
        operator: "Exists"
        effect: "NoExecute"
        tolerationSeconds: 30
      
      initContainers:
      - name: model-downloader
        image: curlimages/curl:latest
        # Add resource limits to prevent init container from consuming too many resources
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "500m"
        command: ['sh', '-c']
        args:
          - |
            set -e
            MODEL_FILE="/models/model.gguf"
            MODEL_URL="https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q4_K_M.gguf"
            
            if [ -f "$MODEL_FILE" ]; then
              echo "Model already exists at $MODEL_FILE ($(ls -lh "$MODEL_FILE" | awk '{print $5}'))"
              exit 0
            fi
            
            echo "Model not found. Downloading from HuggingFace..."
            echo "This may take several minutes for a ~4GB file..."
            
            # Download with progress and resume capability
            curl -L --fail --show-error --progress-bar \
                 --connect-timeout 30 \
                 --max-time 1200 \
                 -C - \
                 -o "$MODEL_FILE.tmp" \
                 "$MODEL_URL"
            
            # Atomic move to prevent partial files
            mv "$MODEL_FILE.tmp" "$MODEL_FILE"
            echo "Download complete! Final size: $(ls -lh "$MODEL_FILE" | awk '{print $5}')"
            
            # Verify file is not empty/corrupted
            if [ ! -s "$MODEL_FILE" ]; then
              echo "ERROR: Downloaded file is empty!"
              exit 1
            fi
        volumeMounts:
        - name: model-storage
          mountPath: /models
      
      containers:
      - name: llamacpp-server
        image: DOCKER_HUB_USERNAME/llamacpp-server:IMAGE_TAG
        imagePullPolicy: IfNotPresent
        
        # Resource limits for predictable scheduling
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        
        ports:
        - containerPort: 8084
          name: http
        
        env:
        - name: PORT
          value: "8084"
        - name: HOST
          value: "0.0.0.0"
        
        # Health checks - adjust paths based on your llamacpp server endpoints
        livenessProbe:
          httpGet:
            path: /health
            port: 8084
          initialDelaySeconds: 60  # Give time for model loading
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /health
            port: 8084
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        
        # Startup probe - critical for slow-loading ML models
        startupProbe:
          httpGet:
            path: /health
            port: 8084
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 60  # 60 * 10s = 10 minutes max startup time
        
        volumeMounts:
        - name: config
          mountPath: /app/config
          readOnly: true
        - name: model-storage
          mountPath: /models
          readOnly: true  # Models should be read-only after download
        
        # Graceful shutdown for model cleanup
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 15"]
      
      volumes:
      - name: config
        configMap:
          name: llamacpp-config
      - name: model-storage
        persistentVolumeClaim:
          claimName: model-storage
      
      # Faster pod startup and termination
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      
      # Ensure pods are distributed across nodes if possible
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - llamacpp
              topologyKey: kubernetes.io/hostname: apps/v1
kind: Deployment
metadata:
  name: llamacpp
  namespace: llama-app
  labels:
    app: llamacpp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: llamacpp
  template:
    metadata:
      labels:
        app: llamacpp
    spec:
      initContainers:
      - name: model-downloader
        image: curlimages/curl:latest
        command: ['sh', '-c']
        args:
          - |
            if [ ! -f /models/model.gguf ]; then
              echo "Model not found. Downloading..."
              curl -L -o /models/model.gguf \
                "https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q4_K_M.gguf"
              echo "Download complete!"
            else
              echo "Model already exists at /models/model.gguf"
            fi
            ls -lh /models/
        volumeMounts:
        - name: model-storage
          mountPath: /models
      containers:
      - name: llamacpp-server
        image: DOCKER_HUB_USERNAME/llamacpp-server:IMAGE_TAG
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8084
          name: http
        env:
        - name: PORT
          value: "8084"
        - name: HOST
          value: "0.0.0.0"
        volumeMounts:
        - name: config
          mountPath: /app/config
          readOnly: true
        - name: model-storage
          mountPath: /models
          readOnly: false
      volumes:
      - name: config
        configMap:
          name: llamacpp-config
      - name: model-storage
        persistentVolumeClaim:
          claimName: model-storage
